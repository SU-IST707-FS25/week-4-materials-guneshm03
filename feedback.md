# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** guneshm03
**Raw Score:** 42/50 (84.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (3/10 = 30.0%)

**Part ex1-part1** (ex1-part1.code): 0/4 points

_Feedback:_ Submission contains only placeholder text - no actual student work provided.

**Part ex1-part2** (ex1-part2.code): 0/3 points

_Feedback:_ Exercise 2 is empty/placeholder—no KNN on t-SNE or performance reported, so 0 points. For credit: compute t-SNE on your data, split train/test, fit KNeighborsClassifier, predict on test, and report accuracy. Your UMAP work applies to Exercise 3.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct use of UMAP for dimensionality reduction and KNN for classification; accuracy computed appropriately. Assumes data and imports exist elsewhere, which is acceptable. Minor note: ensure `time` is imported if not already. Nice work.

---

### Exercise 4 (19/20 = 95.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Good job. You correctly applied PCA (various n_components), trained KNN on the PCA-transformed training data, evaluated on test, and provided a 2D PCA visualization. For the plot, consider using only training data to mirror the modeling step, but your approach is valid.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Excellent. You correctly applied UMAP (fit on train, transform test), explored multiple configs, trained KNN, reported accuracy, and provided a clear 2D visualization. Approach aligns with prior PCA workflow. No issues noted.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ Clear, coherent explanation tied to your results. You compared PCA vs UMAP sensibly and discussed why PCA(3D) retained the key signal. To reach full credit, note that in low dimensions UMAP often improves with smaller n_neighbors; discuss that tradeoff vs your outcomes.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied PCA to 2 components and produced a clear 2D scatter colored by digit class with an appropriate colormap and labels. This meets the requirements and should run given prior imports. Full credit.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Excellent. You fit PCA with 40 components and plotted the explained_variance_ratio_ (converted to percent) for components 1–40. The scree plot meets the requirements and is clearly labeled. No issues.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct and complete. You fit PCA on the training set, computed cumulative explained variance, identified the minimum components to reach 95%, and visualized it. This meets the objective and is consistent with prior work. Nice job.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Good work: you used n_components_95, kept the same digit (index 0), and visualized its position in the reduced PCA space, highlighting it clearly. As an enhancement, you could also inverse_transform and plot the reconstructed digit image to compare with the original.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Excellent: you trained KNN with and without PCA, preserved 80% variance via PCA(n_components=0.80), transformed train/test correctly, and compared accuracies. Timing comparison is a nice bonus. Just ensure PCA is imported in prior cells.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*