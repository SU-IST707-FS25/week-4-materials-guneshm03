# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** guneshm03
**Total Score:** 19/40 (47.5%)

**Grade Category:** F (Failing)

---

## Problem Breakdown

### Exercise 1 (7/16 = 43.8%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good job: you applied PCA to 2D and visualized digits with a clear scatter plot and legend. For a stronger analysis, consider showing explained variance (pca.explained_variance_ratio_) and/or reconstructed images for different component counts.

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You performed PCA and plotted a scree plot, but the task required reducing to 2D and visualizing with a scatter plot colored by class labels. Use PCA(n_components=2), transform the data, and create a 2D scatter with c=y_mnist_train. Avoiding imports penalties.

**Part pipeline-part3** (pipeline-part3.code): 2/4 points

_Feedback:_ Your code runs and computes/plots variance explained, but it’s a cumulative curve, not a scree plot. The task asked for the first 40 components’ individual percent variance explained. Use pca.explained_variance_ratio_[:40]*100 and plot those values (non-cumulative).

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ You correctly used n_components_95 from your prior step to select the components that reach 95% variance. This satisfies the requirement. The additional reduction and 2D visualization are fine. Optionally printing n_components_95 here would make the result explicit.

**Part pipeline-part5** (pipeline-part5.code): 0/4 points

_Feedback:_ This code does not address Step 5. You trained KNN with/without PCA, but the task was to visualize the same digit using the PCA dimensionality from Step 4. Use your n_components_95 PCA, transform the chosen digit, optionally inverse_transform it, and plot with plot_mnist_digit or

---

### Exercise 2 (3/10 = 30.0%)

**Part ex1-part1** (ex1-part1.code): 0/4 points

_Feedback:_ Submission contains only placeholder text - no actual student work provided.

**Part ex1-part2** (ex1-part2.code): 0/3 points

_Feedback:_ Exercise 2 is empty/placeholder only. You didn’t implement a KNN classifier on t-SNE-transformed data or report its performance. Please add code to fit t-SNE on your data, split, train KNN, predict, and print accuracy.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you reduced with UMAP, trained KNN, and computed accuracy correctly on test data. Clean, valid approach and consistent variable use. Minor note: ensure time is imported if timing is required.

---

### Exercise 4 (9/14 = 64.3%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ You correctly explored PCA (1–3 comps) with KNN and a 2D plot. However, you didn’t implement UMAP or vary its parameters as required, and the second cell is empty. To complete: add UMAP with different n_components, n_neighbors, min_dist; compare KNN accuracies and visualizations.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP+KNN instead of the requested PCA+KNN. The structure (fit on train, transform test, evaluate, 2D plot) is solid, but it does not use PCA as specified or as in your prior work. Replace UMAP with PCA to meet the exercise’s goal.

**Part ex2-part3** (ex2-part3.answer): 6/7 points

_Feedback:_ Good conceptual comparison of PCA vs UMAP and why z matters. Clear reasoning about local structure vs variance. However, you don’t reference the actual results you obtained, and you omit discussing your 3D UMAP run’s performance. Add quantitative evidence to strengthen the claim.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:23 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*